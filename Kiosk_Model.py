# -*- coding: utf-8 -*-
"""Kiosk_LUNG_Vrushali2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CjURIPB0fEfAqe_R6gPgEiNCU8MTFnGV
"""

# Load various imports
from datetime import datetime
from os import listdir
from os.path import isfile, join

import librosa
import librosa.display
from tqdm import tqdm

import numpy as np
import pandas as pd

import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D
from keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint

from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.preprocessing import MinMaxScaler

!pip install pydub

!pip install resampy

!pip install librosa --upgrade

from google.colab import drive
drive.mount('/content/drive')

mypath = "/content/drive/MyDrive/Project/Dataset/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files"
filenames = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and f.endswith('.wav'))]

p_id_in_file = [] # patient IDs corresponding to each file
for name in filenames:
    p_id_in_file.append(int(name[:3]))

p_id_in_file = np.array(p_id_in_file)

filepaths = [join(mypath, f) for f in filenames] # full paths of files

p_diag = pd.read_csv("/content/drive/MyDrive/Project/Dataset/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv",header=None) # patient diagnosis file
p_diag

labels = np.array([p_diag[p_diag[0] == x][1].values[0] for x in p_id_in_file]) # labels for audio files

def add_noise(data):
    noise_value = 0.015 * np.random.uniform() * np.amax(data)
    data = data + noise_value * np.random.normal(size=data.shape[0])
    return data

def stretch_process(data, rate=0.8):
    return librosa.effects.time_stretch(data, rate=0.8)

def pitch_process(data, sampling_rate, pitch_factor=0.7):
    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)

def extract_process(data, sample_rate):
    output_result = np.array([])

    # Zero Crossing Rate
    mean_zero = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)
    output_result = np.hstack((output_result, mean_zero))

    # Chroma STFT
    stft_out = np.abs(librosa.stft(data))
    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft_out, sr=sample_rate).T, axis=0)
    output_result = np.hstack((output_result, chroma_stft))

    # MFCC
    mfcc_out = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40).T, axis=0)
    output_result = np.hstack((output_result, mfcc_out))

    # RMS Energy
    root_mean_out = np.mean(librosa.feature.rms(y=data).T, axis=0)
    output_result = np.hstack((output_result, root_mean_out))

    # Mel Spectrogram
    mel_spectogram = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)
    output_result = np.hstack((output_result, mel_spectogram))

    # Spectral Contrast
    spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=data, sr=sample_rate).T, axis=0)
    output_result = np.hstack((output_result, spectral_contrast))

    return output_result

# from google.colab import drive
# drive.mount('/content/drive')

def extract_features(file_name):
    """
    This function takes in the path for an audio file as a string, loads it,
    and extracts several audio features including mean values of MFCC, Zero Crossing Rate,
    Chromagram, Root Mean Square Energy, and Mel Spectrogram from the original and its augmented versions.
    """
    try:
        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast', duration=42, offset=0.6)

        # Extract features from the original audio data
        extracted_features = extract_process(audio, sample_rate)
        result = np.array(extracted_features)

        # Add noise and extract features
        noise_out = add_noise(audio)
        output_2 = extract_process(noise_out, sample_rate)
        result = np.vstack((result, output_2))

        # Time-stretch and then pitch-shift before extracting features
        new_out = stretch_process(audio,0.8)
        stretch_pitch = pitch_process(new_out, sample_rate,pitch_factor=0.7)
        output_3 = extract_process(stretch_pitch, sample_rate)
        result = np.vstack((result, output_3))

    except Exception as e:
        print("Error encountered while parsing file: ", file_name)
        print("Error Details:", e)
        return None

    return result

features = []
counter = 0
# Iterate through each sound file and extract the features
for file_name in tqdm(filepaths, desc='Extracting features'):
    if counter == 500:
      break;
    data = extract_features(file_name)
    features.append(data)
    counter+=1

print('Finished feature extraction from ', len(features), ' files')
features = np.array(features)
features

features.shape,labels.shape

# delete the very rare diseases
# import numpy as np
# mask = np.where((labels == 'Asthma') | (labels == 'LRTI'))[0]
# print(mask)
# new_features = np.delete(features, mask, axis=0)

# new_labels = np.delete(labels, mask, axis=0)
new_features = features
new_labels = labels[:500]

new_features.shape,new_labels.shape

flattened_features = new_features.reshape((-1, 189))

augmented_labels = np.repeat(new_labels, 3)

# augmented_features = np.reshape(features, (-1, 182))  # Flatten the feature sets

mask = ~np.isin(augmented_labels, ['Asthma', 'LRTI'])
augmented_features = flattened_features[mask]
augmented_labels = augmented_labels[mask]

unique_values = set(augmented_labels)
unique_values

augmented_labels.shape,augmented_features.shape

import pandas as pd
lab = pd.Series(augmented_labels)
lab.value_counts()

from imblearn.over_sampling import SMOTE

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
balanced_features, balanced_labels = smote.fit_resample(augmented_features, augmented_labels)
LE = LabelEncoder()
encoded_labels = LE.fit_transform(balanced_labels)
categoriocal_labels = to_categorical(encoded_labels)
# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(balanced_features, categoriocal_labels, test_size=0.2, random_state=42)

scaler_data = StandardScaler()

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

y_train[:10]

x_train = scaler_data.fit_transform(x_train)
x_test = scaler_data.transform(x_test)

x_train = np.expand_dims(x_train,axis=2)
x_test = np.expand_dims(x_test,axis=2)

from tensorflow.keras import layers, Sequential

num_labels = len(np.unique(balanced_labels))  # Determine the number of unique classes
print(num_labels)
Model = Sequential([
    layers.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=5, strides=2, padding='same'),
    layers.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=5, strides=2, padding='same'),
    layers.Dropout(0.3),

    layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=5, strides=2, padding='same'),
    layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=5, strides=2, padding='same'),
    layers.Dropout(0.3),

    layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=5, strides=2, padding='same'),
    layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=5, strides=2, padding='same'),
    layers.Dropout(0.3),

    layers.GlobalAveragePooling1D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.4),
    layers.Dense(num_labels, activation='softmax')
])
early_stop = tensorflow.keras.callbacks.EarlyStopping(monitor="loss",patience=3,mode="min")

Model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = Model.fit(x_train, y_train, batch_size=32, epochs=4, validation_data=(x_test, y_test),callbacks=[early_stop])

Model_Results = Model.evaluate(x_test,y_test)
print("LOSS:  " + "%.4f" % Model_Results[0])
print("ACCURACY:  " + "%.4f" % Model_Results[1])

# plt.figure(figsize=(14, 5))
# plt.subplot(1, 2, 1)
# plt.plot(history.history['loss'], label='Training Loss')
# plt.plot(history.history['val_loss'], label='Validation Loss')
# plt.legend()
# plt.title('Training and Validation Loss')

# # Plotting accuracy
# plt.subplot(1, 2, 2)
# plt.plot(history.history['accuracy'], label='Training Accuracy')
# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
# plt.legend()
# plt.title('Training and Validation Accuracy')

# plt.show()

# Getting the predictions
y_pred = Model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

confusion_mtx = confusion_matrix(y_true, y_pred_classes)
le = LE
cm_display = ConfusionMatrixDisplay(confusion_mtx, display_labels=le.classes_).plot()

from sklearn.metrics import accuracy_score
def predict(x_test):
    y_pred = Model.predict(x_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true = np.argmax(y_test, axis=1)
    mapping = {
        0: 'Bronchiectasis',
        1: 'Bronchiolitis',
        2: 'COPD',
        3: 'Healthy',
        4: 'Pneumonia',
        5: 'URTI'
    }

    # Replace numerical labels with original labels
    predictions = [mapping[label] for label in y_pred_classes]
    true_vals = [mapping[label] for label in y_true]

    accuracy = accuracy_score(y_true, y_pred_classes)
    print("Test Accuracy:", accuracy)

    return predictions,true_vals

predictions,true_vals = predict(x_test)
list(zip(predictions,true_vals))

from tensorflow.keras.models import save_model
import os

model_path = "/content/drive/MyDrive/model"

if not os.path.exists(model_path):
    os.makedirs(model_path)

Model.save(os.path.join(model_path, "Diagnosis.h5"))

print("Model saved successfully. You can download it from the following link:")
print(os.path.join(model_path, "Kiosk_Model2.h5"))

pip install --upgrade jupyter nbconvert

!jupyter nbconvert --to html /content/Kiosk_LUNG_Vrushali2.ipynb --log-level=DEBUG

